---
description: LLM Quest Benchmart Project
globs: 
---
################################################################################
#  Rules for llm-quest-benchmark project     #
################################################################################

PROJECT_NAME: llm-quest-benchmark
GOAL: Benchmark LLM performance on Space Rangers text quests; build a robust evaluation framework. Target: Next Week.
VERSION: 0.2.0

# Core Principles (Specific to this project)
PROJECT_PRINCIPLES:
  - QM_FIRST: Focus on Space Rangers quests.
  - DEVELOPER_HAPPINESS: Prioritize interesting tech and rapid iteration.
  - MINIMAL_TEXTARENA: Use TextArena for infrastructure, but keep custom components focused.

# Key Files/Directories (Update this as your project evolves)
# This section provides a high-level overview, not an exhaustive list.
CONTEXT_FILES:
  - README.md: Project overview and setup.
  - roadmap.md: Project roadmap.
  - llm_quest_benchmark/scripts/llm_quest.py: Main benchmarking script.
  - space-rangers-quest/src/lib/qmreader.ts: QM parser (TypeScript).
  - llm_quest_benchmark/agents/llm_agent.py: Example LLM agent.

# Tools and Commands (Specific to this project)
# Provide concise descriptions and usage examples.
TOOLS:
  - llm-quest: Run a quest and benchmark an LLM agent.
    - Usage: `llm-quest --quest <quest_path> --log-level <level>`
  - llm-analyze: Analyze benchmark results.
    - Usage: `llm-analyze <metrics_file>`
  - qm_player.py: Play quests interactively.
    - Usage: `python llm_quest_benchmark/scripts/qm_player.py <quest_path>`
  - npm run pack-game-data (in space-rangers-quest): Pack game data.
    - Usage: `cd space-rangers-quest; npm run pack-game-data`
  - pre-commit: Maintain code quality
    - Usage: `pre-commit run --all-files`
  - format: Auto-format code
    - Usage: `yapf -irp . && isort .`

# Development Standards
STANDARDS:
  - CODE_STYLE: All code must pass pre-commit checks
  - TYPING: Use type hints for all public interfaces
  - DOCS: Keep docs updated with code changes